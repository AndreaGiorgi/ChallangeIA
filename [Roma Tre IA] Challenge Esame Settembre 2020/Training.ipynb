{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inizializzazione del dataset\n",
    "\n",
    "Il dataset è composto da due sotto file in formato .csv:\n",
    "1. trainingData, è un dataframe pandas creato a partire dal file di training \"train.csv\"\n",
    "2. testData, è un dataframe pandas creato a partire dal file di testing \"test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv('train.csv')\n",
    "testData = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controllo iniziale del TrainData set\n",
    "\n",
    "Una volta caricato il TrainData set controllo che i dati presenti all'interno siano corretti, ovvero che non vi siano campi nulli o NaN. In questo caso la ricerca dei duplicati è futile poiché non abbiamo conoscenza sull'identità del paziente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = trainData.isnull()\n",
    "for column in trainData:\n",
    "    print(column)\n",
    "    print(missing_data[column].value_counts())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_data = trainData.isna()\n",
    "for column in trainData:\n",
    "    print(column)\n",
    "    print(NaN_data[column].value_counts())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controllo iniziale del TestData set\n",
    "\n",
    "Una volta caricato il TrainData set controllo che i dati presenti all'interno siano corretti, ovvero che non vi siano campi nulli o NaN. In questo caso la ricerca dei duplicati è futile poiché non abbiamo conoscenza sull'identità del paziente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = testData.isnull()\n",
    "for column in testData:\n",
    "    print(column)\n",
    "    print(missing_data[column].value_counts())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_data = testData.isna()\n",
    "for column in testData:\n",
    "    print(column)\n",
    "    print(NaN_data[column].value_counts())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controllo dettagliato del trainData set\n",
    "\n",
    "Una volta controllata la presenza di valori nulli o duplicati all'interno del dataset è possibile analizzarne nel dettaglio le varie colonne. Questa analisi supporta la successiva operazione di feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(trainData.columns):\n",
    "    plt.figure(i)\n",
    "    sns.lineplot(x='DEATH_EVENT',y=col, data=trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(trainData.corr(), annot=True, fmt='.1g');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = trainData.corr()\n",
    "correlation_target = abs(correlation[\"DEATH_EVENT\"])\n",
    "features = correlation_target[correlation_target > 0.1]\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analisi della heatmap e della corrleazione tra le varie condizioni mediche del paziente fanno emergere un forte collegamento tra la possibile morte durante il periodo di osservazione con le seguenti caratteristiche:\n",
    "1. L'età del paziente\n",
    "2. La Ejection Fraction, ovvero la frazione di eiezione è la frazione volumetrica del fluido espulsa da una camera ad ogni contrazione, un valore normale è compreso tra 50 e 75 \n",
    "3. la concentrazione nel siero di creatina e sodio\n",
    "4. Il tempo di osservazione, più il paziente è sotto osservazione e più è probabile che sia posssibile tenere traccia di un possibile un arresto cardiaco\n",
    "\n",
    "L'analisi dei grafici valida questa decisione, inoltre si nota come la feature \"smoking\" e \"sex\" potrebbero essere correlate tra loro, ovvero che l'incidenza di arresti cardiaci potrebbe anche essere collegata al binomio genere-fumatore\n",
    "\n",
    "Istanzio una struttura dati chiamata selectedFeatures all'interno della quale inserisco le label delle feature scelte in seguito all'analisi dei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\"age\", \"creatinine_phosphokinase\", \"ejection_fraction\", \"serum_creatinine\", \"serum_sodium\", \"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Dall'analisi del dataset si nota come vi siano delle colonne che usano scale diverse, è conveniente riportarle tutte nella stessa scala di misura. Nello specifico le colonne in questione sono:\n",
    "1. platelets\n",
    "2. creatinine_phosphokinase\n",
    "3. ejection_fraction\n",
    "4. serum_sodium\n",
    "\n",
    "La colonna time viene lasciata così com'è poiché rappresenta il numero di giorni in cui il paziente è stato ricoverato.\n",
    "\n",
    "Per allineare la loro scala di misura sfrutto la funzionalità offerta di sklearn MinMaxScaler. Il suo funzionamento si basa sulla definizione di un range all'interno del quale andare a scalare i valori delle colonne passate come input. Il range di default è [0,1]\n",
    "\n",
    "Transform è il contenitore della nostra trasformazione, vengono definiti al suo interno:\n",
    "1. Nome trasformazione\n",
    "2. La funzione di trasformazione, nel nostro caso MinMaxScaler()\n",
    "3. Le colonne interessate dalla nostra trasformazione\n",
    "\n",
    "Inoltre viene utilizzata la ColumnTransformer, la quale permette di applicare MinMaxScaler alle varie colonne, supportando la funzionalità di passthrough, ovvero la possibilità di tralasciare dalla trasformazione le colonne non specificate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = [\n",
    "    ['MinMaxScaler', MinMaxScaler(),\n",
    "     ['platelets','creatinine_phosphokinase','ejection_fraction','serum_sodium']]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformFunction = ColumnTransformer(transform, remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledTrainingSet = pd.DataFrame(transformFunction.fit_transform(trainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledTestSet = pd.DataFrame(transformFunction.fit_transform(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledTrainingSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledTestSet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può notare l'operazione di MinMax ha effettivamente riportato i valori in un intervallo [0,1], tuttavia l'operazione ha eliminato i nomi delle colonne ed ha concatenato le colonne trasformate davanti a quelle tralasciate dall'operazione. Dobbiamo necessariamente ripristinare le labels delle features per poterle utilizzare nel mnostro modello. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledTrainingSet.columns=['platelets','creatinine_phosphokinase','ejection_fraction','serum_sodium','DEATH_EVENT', 'age', 'anaemia', 'diabetes', 'high_blood_pressure', 'serum_creatinine', 'sex', 'smoking', 'time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledTestSet.columns=['platelets','creatinine_phosphokinase','ejection_fraction','serum_sodium','DEATH_EVENT','age', 'anaemia', 'diabetes', 'high_blood_pressure', 'serum_creatinine', 'sex', 'smoking', 'time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inizializzazione Modello: RandomForest\n",
    "\n",
    "Il modello scelto è un classificatore RandomForest. Il classificatore RandomForest utilizza diversi classificatori Decision Tree su diversi sotto campioni del dataset di input. Ogni classificatore compierà un'operazione di fitting sul suo sottocampione. Il classificatore RandomForest utilizzerà una funzione di averaging, ovvero fa la media tra le varie predizioni, per migliorare l'accuratezza e tenere sotto controllo l'overfitting. \n",
    "\n",
    "Un Decision tree classifier ha il compito di predirre il valore della variabile target, nel nostro caso DEATH_EVENT, apprendendo delle regole di decisione a partire dalle feature del nostro dataset\n",
    "\n",
    "Il classificatore viene poi valutato in accuratezza, tramite la metrica accuracy_score, sia sul dataset di training che su quello di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrain = trainData[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTest = testData[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = trainData[\"DEATH_EVENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = testData[\"DEATH_EVENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=10, n_estimators=32, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(datasetTrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_TrainingSet = clf.predict(datasetTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_TestSet = clf.predict(datasetTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuratezza\n",
    "\n",
    "L'accuratezza viene calcolata usando le previsioni prodotte dal modello sia sul trainingSet che sul testSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set accuracy: {:.2f}\".format(clf.score(datasetTrain, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set accuracy: {:.2f}\".format(accuracy_score(y_test, predict_TestSet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, predict_TestSet),annot=True)\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nostro dataset è caratterizzato da una ridotta quantità di dati, questo comporta un rischio di overfitting che potrebbe inficiare le prestazioni del nostro modello. Per controllarne la validità possiamo effettuare un controllo sulla sua accuratezza sfruttando la K-Fold Cross Validation, nello specifico la 10-Fold Cross Validation.\n",
    "\n",
    "Inoltre è da notare come il dataset sia sbilanciato verso i dati relativi alle persone che non hanno subito arresti cardiaci, ovvero la presenza di dati relativi ai decessi è molto esigua e questo potrebbe inficiare sul processo di addestramento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, datasetTest, y_test, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy using K-Fold Cross Validation, with k=10 : %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
